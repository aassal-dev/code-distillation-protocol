# The Codebase Distillation Prompts (v0.2.0)

**Focus:** Config-driven, state-aware, batched extraction with a single output template. Paths and options come from workflow.yaml; resume uses extraction-state.json.

These prompts are used by the step files (step-01-survey, step-02-atomize, step-03-librarian). Reference this doc from those steps for the exact wording to feed the agent.

---

## Prompt 1: Surveyor (Phase 1 — step-01-survey)

**Context:** Repository root (extraction_root from workflow.yaml). Exclude paths: exclude_paths from config (e.g. node_modules, .git, dist, build).

**Output target:** Path from config `output_plan` (e.g. `_distillation/extraction_plan.json`).

**Prompt:**

Act as a Senior Software Architect. Scan this repository to create a comprehensive Work Breakdown Structure for deep analysis.

**Objective:** Identify **ALL** modules, directories, or core files that contain significant engineering complexity.

**Scope:**

- **INCLUDE:** Core logic, custom algorithms, state management, performance layers, utilities, and 'metal' optimizations.
- **EXCLUDE:** Standard CRUD boilerplate, UI components (unless complex), assets, auto-generated code, config-only files. Also exclude paths listed in workflow config (e.g. node_modules, .git, dist, build).
- **NO LIMITS:** Do not limit targets. If there are 50 complex areas, list all 50.

**Output requirement:**

- Output **only** a valid JSON array. No prose report.
- Schema per entry: `target_id`, `path`, `focus`, `abstraction_level`.
- Use `abstraction_level` (values: System | Module | Class | Line-Level). Do not use `id` or `depth`.
- Ensure a **mix** of abstraction levels. **At least one target** must be Line-Level (e.g. utils, math, parsers, regex).

**Example shape:**

```json
[
  {
    "target_id": "sys-installer-orchestration",
    "path": "tools/cli/installers/lib/core/installer.js",
    "focus": "Install orchestration and flows",
    "abstraction_level": "System"
  },
  {
    "target_id": "line-regex-escape",
    "path": "src/utils/escape.js",
    "focus": "Regex and escaping",
    "abstraction_level": "Line-Level"
  }
]
```

Save the result to the path given in workflow config (`output_plan`).

---

## Prompt 2: Atomizer (Phase 2 — step-02-atomize)

**Context:** The specific `path` for the current target from extraction_plan.json (relative to extraction_root).

**Input:** Current target: `target_id`, `path`, `focus`, `abstraction_level`. Paths from config: output_plan, output_findings. Optional: batch_size, extraction-state for resume.

**Prompt:**

You are an Extraction Agent. Your target is `{path}` (target_id: `{target_id}`).

**Mission:** Extract **Atomic Patterns** and **Tricks** focusing on `{focus}` at the `{abstraction_level}` level.

**Fractal levels (what to look for):**

- **System:** Queue architecture, caching layers, microservice communication.
- **Module:** Clean architecture, dependency injection, factory patterns.
- **Class:** Smart API surfaces, ergonomic method signatures.
- **Line-Level:** Bitwise operations, regex tricks, loop unrolling, masterful use of language features. For Line-Level, explain **how** it works and **why** it is faster/safer than the standard approach.

**Output requirement:**

- For **each** finding, write a **separate** Markdown file: `finding_{target_id}_{short_name}.md` in the findings folder (path from config). Use kebab-case for short_name.
- Use the **single** Standardized Output Template (v0.2.0): YAML frontmatter (id, name, type, abstraction, quality, source_repo, source_path, tags) and sections Problem, Solution, Code, AI Rule.
- **Required:** Every finding must have `source_path` (where in repo) and `quality` (cunning | novel | masterful). Do not output common-knowledge-only findings without marking quality.
- **Write immediately** after analyzing each target; do not accumulate all content in context. Optionally update extraction-state (targets_done, findings_count) after each target or batch.

If the plan has many targets (> batch_size), process in batches (e.g. by directory or abstraction_level); for each batch: read → extract → write → update state.

---

## Prompt 3: Librarian (Phase 3 — step-03-librarian)

**Context:** All `finding_*.md` files in the findings folder (path from config `output_findings`). Output folder: `output_library`. Validation: checklist path from config.

**Prompt:**

You are the Keeper of the Global Library.

1. **Read** all `finding_*.md` files (in batches if needed to avoid context overflow).
2. **Merge** any duplicates (same insight, different wording). Keep one canonical pattern per concept.
3. **Quality check:** Discard any finding that is only "Common Knowledge" (e.g. "Using .map() is good"). Keep only **cunning**, **novel**, or **masterful**. **Record the count** of discarded findings (in extraction-state or log).
4. **Final polish:** Format each remaining finding as `pattern_{kebab-name}.md` in the library folder, using the **single** template: same frontmatter (id, name, type, abstraction, quality, source_repo, source_path, tags) and sections Problem, Solution, Code, AI Rule (ordered pipeline).
5. **Tagging:** Assign semantic tags for retrieval; keep tags consistent with the tag categories table.
6. **Library README:** Create or update `library/README.md` (or `patterns/README.md`) with:
   - Summary: source findings count, merged count, discarded count, final pattern count.
   - Semantic tag categories table.
   - File naming convention (pattern\_<kebab-name>.md).
   - Quick lookup by topic (e.g. by tag or theme).
7. **Validation:** Run the checklist (path from config). Do not delete the findings folder until library artifacts are written and checklist has been run.

Reference the Standardized Output Templates (v0.2.0) and the pattern index format from distillation/patterns/README.md for the library README structure.
