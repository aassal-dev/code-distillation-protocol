# The Codebase Distillation Prompts (v0.2.05)

**Focus:** Config-driven, state-aware, batched extraction with optional Director (Phase 0), learning_objectives, and optional Blueprint/Theory vs. Practice. Paths and options come from workflow.yaml; resume uses extraction-state.json. v0.2.05 adds mandate block, write-as-you-go, and post–Phase 1 ask (c/p/y).

These prompts are used by the step files (step-00-director, step-01-survey, step-02-atomize, step-03-librarian). Reference this doc from those steps for the exact wording to feed the agent.

---

## Prompt 0: Director (Phase 0 — step-00-director, when use_director: true)

**Context:** Repository root (README, docs); optional Blueprint/theory file path from config (blueprint_file or theory_context_path).

**Output target:** Path from config `output_objectives` (e.g. `_distillation/learning_objectives.md`). Optional: implementation_map.json when Blueprint provided.

**Prompt:**

Act as the Lead Researcher. Before we scan the code, we need to know what we are hunting for.

**Task:**

1. Read the README.md and any user-facing documentation. If a theory/Blueprint file path is provided in config, read it and use it to shape learning goals.
2. Analyze the core value proposition of this repository. What does this repo do exceptionally well?
3. Formulate 3–7 explicit **Learning Targets**: specific architectural or algorithmic questions the next phase will answer by reading the code (e.g. "How does the Context Engine manage token limits during graph traversal?").

**Output:** Generate learning_objectives.md at the path from config (output_objectives). List 3–7 Learning Targets. When a Blueprint was provided and implementation mapping is requested, optionally produce implementation_map.json (theory concept → file path / line) in \_distillation.

Use only the paths from workflow.yaml; do not hardcode.

---

## Prompt 1: Surveyor (Phase 1 — step-01-survey)

**Context:** Repository root (extraction_root from workflow.yaml). When learning_objectives.md exists at output_objectives, read it first. Exclude paths: exclude_paths from config (e.g. node_modules, .git, dist, build).

**Output target:** Path from config `output_plan` (e.g. `_distillation/extraction_plan.json`).

**Prompt:**

Act as a Senior Software Architect.

- **When learning_objectives.md exists:** We have established learning goals in that file. Identify the specific files and directories that hold the answers to those Learning Targets. Cross-reference the Learning Targets with the file tree. Set each plan entry's `focus` to reference one or more of those targets.
- **When learning_objectives.md does not exist:** Scan this repository to create a comprehensive Work Breakdown Structure for deep analysis. Identify ALL modules, directories, or core files that contain significant engineering complexity (nerve centers by complexity).

**Scope:**

- **INCLUDE:** Core logic, custom algorithms, state management, performance layers, utilities, and 'metal' optimizations.
- **EXCLUDE:** Standard CRUD boilerplate, UI components (unless complex), assets, auto-generated code, config-only files. Also exclude paths listed in workflow config (e.g. node_modules, .git, dist, build).
- **NO LIMITS:** Do not limit targets. If there are 50 complex areas, list all 50.

**Output requirement:**

- Output **only** a valid JSON array. No prose report.
- Schema per entry: `target_id`, `path`, `focus`, `abstraction_level`.
- Use `abstraction_level` (values: System | Module | Class | Line-Level). Do not use `id` or `depth`.
- Ensure a **mix** of abstraction levels. **At least one target** must be Line-Level (e.g. utils, math, parsers, regex).

Save the result to the path given in workflow config (`output_plan`).

---

## Prompt 2: Atomizer (Phase 2 — step-02-atomize)

**Context:** The specific `path` for the current target from extraction_plan.json (relative to extraction_root). When implementation_map.json exists (Blueprint run), use it to add "Theory vs. Practice" where the target maps to a theory concept.

**Input:** Current target: `target_id`, `path`, `focus`, `abstraction_level`. Paths from config: output_plan, output_findings. Optional: batch_size, extraction-state for resume.

**Prompt:**

You are an Extraction Agent. Your target is `{path}` (target_id: `{target_id}`).

**Mission:** Extract **Atomic Patterns** and **Tricks** focusing on `{focus}` at the `{abstraction_level}` level.

**Fractal levels (what to look for):**

- **System:** Queue architecture, caching layers, microservice communication.
- **Module:** Clean architecture, dependency injection, factory patterns.
- **Class:** Smart API surfaces, ergonomic method signatures.
- **Line-Level:** Bitwise operations, regex tricks, loop unrolling, masterful use of language features. For Line-Level, explain **how** it works and **why** it is faster/safer than the standard approach.

**Output requirement:**

- For **each** finding, write a **separate** Markdown file: `finding_{target_id}_{short_name}.md` in the findings folder (path from config). Use kebab-case for short_name.
- Use the **single** Standardized Output Template (v0.2.05): YAML frontmatter (id, name, type, abstraction, quality, source_repo, source_path, tags) and sections Problem, Solution, Code, AI Rule.
- **When Blueprint was used** and this target maps to a theory concept: add a **"Theory vs. Practice"** section—explain how the textbook theory was adapted for real-world production in this codebase.
- **Required:** Every finding must have `source_path` (where in repo) and `quality` (cunning | novel | masterful). Do not output common-knowledge-only findings without marking quality.
- **Write immediately** after analyzing each target; do not accumulate all content in context. Update extraction-state (targets_done, findings_count) after each target or batch.

If the plan has many targets (> batch_size), process in batches (e.g. by directory or abstraction_level); for each batch: read → extract → write → update state.

---

## Prompt 3: Librarian (Phase 3 — step-03-librarian)

**Context:** All `finding_*.md` files in the findings folder (path from config `output_findings`). Output folder: `output_library`. Validation: checklist path from config. Preserve "Theory vs. Practice" sections when present in findings.

**Prompt:**

You are the Keeper of the Global Library.

1. **Read** all `finding_*.md` files (in batches if needed to avoid context overflow).
2. **Merge** any duplicates (same insight, different wording). Keep one canonical pattern per concept.
3. **Quality check:** Discard any finding that is only "Common Knowledge" (e.g. "Using .map() is good"). Keep only **cunning**, **novel**, or **masterful**. **Record the count** of discarded findings (in extraction-state or log).
4. **Final polish:** Format each remaining finding as `pattern_{kebab-name}.md` in the library folder, using the **single** template: same frontmatter (id, name, type, abstraction, quality, source_repo, source_path, tags) and sections Problem, Solution, Code, AI Rule. **Preserve** any "Theory vs. Practice" section from the finding.
5. **Tagging:** Assign semantic tags for retrieval; keep tags consistent with the tag categories table.
6. **Library README:** Create or update `library/README.md` (or `patterns/README.md`) with:
   - Summary: source findings count, merged count, discarded count, final pattern count.
   - Semantic tag categories table.
   - File naming convention (pattern\_<kebab-name>.md).
   - Quick lookup by topic (e.g. by tag or theme).
7. **Validation:** Run the checklist (path from config). Do not delete the findings folder until library artifacts are written and checklist has been run.

Reference the Standardized Output Templates (v0.2.05) and the pattern index format from distillation/patterns/README.md for the library README structure.
