# The Codebase Distillation Prompts (v0.2.06)

**Focus:** Config-driven, state-aware, batched extraction with optional Director (Phase 0), learning_objectives, and optional Blueprint/Theory vs. Practice. Paths and options come from workflow.yaml; resume uses extraction-state.json. v0.2.06 adds Code completeness, merge-map, concept_id, proportional Learning Targets, and tagging/README guidelines.

These prompts are used by the step files (step-00-director, step-01-survey, step-02-atomize, step-03-librarian). Reference this doc from those steps for the exact wording to feed the agent.

---

## Prompt 0: Director (Phase 0 — step-00-director, when use_director: true)

**Context:** Repository root (README, docs); optional Blueprint/theory file path from config (blueprint_file or theory_context_path).

**Output target:** Path from config `output_objectives` (e.g. `_distillation/learning_objectives.md`). Optional: implementation_map.json when Blueprint provided.

**Prompt:**

Act as the Lead Researcher. Before we scan the code, we need to know what we are hunting for.

**Task:**

1. Read the README.md and any user-facing documentation. If a theory/Blueprint file path is provided in config, read it and use it to shape learning goals.
2. Analyze the core value proposition of this repository. What does this repo do exceptionally well?
3. Formulate a number of **Learning Targets** **proportional to the size of the code**: specific architectural or algorithmic questions the next phase will answer by reading the code (e.g. "How does the Context Engine manage token limits during graph traversal?"). No fixed minimum or maximum; Surveyor and Atomizer process all listed targets.

**Output:** Generate learning_objectives.md at the path from config (output_objectives). List Learning Targets (count proportional to code size). When a Blueprint was provided and implementation mapping is requested, optionally produce implementation_map.json (theory concept → file path / line) in \_distillation.

Use only the paths from workflow.yaml; do not hardcode.

---

## Prompt 1: Surveyor (Phase 1 — step-01-survey)

**Context:** Repository root (extraction_root from workflow.yaml). When learning_objectives.md exists at output_objectives, read it first. Exclude paths: exclude_paths from config (e.g. node_modules, .git, dist, build).

**Output target:** Path from config `output_plan` (e.g. `_distillation/extraction_plan.json`).

**Prompt:**

Act as a Senior Software Architect.

- **When learning_objectives.md exists:** We have established learning goals in that file. Identify the specific files and directories that hold the answers to those Learning Targets. Cross-reference the Learning Targets with the file tree. Set each plan entry's `focus` to reference one or more of those targets.
- **When learning_objectives.md does not exist:** Scan this repository to create a comprehensive Work Breakdown Structure for deep analysis. Identify ALL modules, directories, or core files that contain significant engineering complexity (nerve centers by complexity).

**Scope:**

- **INCLUDE:** Core logic, custom algorithms, state management, performance layers, utilities, and 'metal' optimizations.
- **EXCLUDE:** Standard CRUD boilerplate, UI components (unless complex), assets, auto-generated code, config-only files. Also exclude paths listed in workflow config (e.g. node_modules, .git, dist, build).
- **NO LIMITS:** Do not limit targets. If there are 50 complex areas, list all 50.

**Output requirement:**

- Output **only** a valid JSON array. No prose report.
- Schema per entry: `target_id`, `path`, `focus`, `abstraction_level` (required). Optional: `concept_id` (short kebab-case, e.g. hitl-async-suspension) when learning_objectives or focus implies a concept for merge traceability.
- Use `abstraction_level` (values: System | Module | Class | Line-Level). Do not use `id` or `depth`.
- Ensure a **mix** of abstraction levels. **At least one target** must be Line-Level (e.g. utils, math, parsers, regex).

Save the result to the path given in workflow config (`output_plan`).

---

## Prompt 2: Atomizer (Phase 2 — step-02-atomize)

**Context:** The specific `path` for the current target from extraction_plan.json (relative to extraction_root). When implementation_map.json exists (Blueprint run), use it to add "Theory vs. Practice" where the target maps to a theory concept.

**Input:** Current target: `target_id`, `path`, `focus`, `abstraction_level`. Paths from config: output_plan, output_findings. Optional: batch_size, extraction-state for resume.

**Prompt:**

You are an Extraction Agent. Your target is `{path}` (target_id: `{target_id}`).

**Mission:** Extract **Atomic Patterns** and **Tricks** focusing on `{focus}` at the `{abstraction_level}` level.

**Fractal levels (what to look for):**

- **System:** Queue architecture, caching layers, microservice communication.
- **Module:** Clean architecture, dependency injection, factory patterns.
- **Class:** Smart API surfaces, ergonomic method signatures.
- **Line-Level:** Bitwise operations, regex tricks, loop unrolling, masterful use of language features. For Line-Level, explain **how** it works and **why** it is faster/safer than the standard approach.

**Output requirement:**

- For **each** finding, write a **separate** Markdown file: `finding_{target_id}_{short_name}.md` in the findings folder (path from config). Use kebab-case for short_name.
- Use the **single** Standardized Output Template (v0.2.06): YAML frontmatter (id, name, type, abstraction, quality, source_repo, source_path, tags; optional code_completeness, concept_id) and sections Problem, Solution, Code, AI Rule.
- **Code section (v0.2.06):** Each finding's **Code** section must contain at least: (a) a minimal working slice (typically 5–15 lines) showing core logic/data flow/API, or (b) a complete small example (one function/schema block) copy-paste adaptable. **Exception:** Code may be comment-only or "see source_path" only if the finding is marked `code_completeness: pointer`. Line-level findings: 1–5 lines of real code plus brief "how" and "why." Set `code_completeness: minimal | full | pointer` in frontmatter when useful.
- **When Blueprint was used** and this target maps to a theory concept: add a **"Theory vs. Practice"** section—explain how the textbook theory was adapted for real-world production in this codebase.
- **Required:** Every finding must have `source_path` (where in repo) and `quality` (cunning | novel | masterful). Do not output common-knowledge-only findings without marking quality.
- **Tagging:** Prefer existing tags from the library README; add new tags only when no existing tag fits.
- **Write immediately** after analyzing each target; do not accumulate all content in context. Update extraction-state (targets_done, findings_count) after each target or batch.

If the plan has many targets (> batch_size), process in batches (e.g. by directory or abstraction_level); for each batch: read → extract → write → update state.

---

## Prompt 3: Librarian (Phase 3 — step-03-librarian)

**Context:** All `finding_*.md` files in the findings folder (path from config `output_findings`). Output folder: `output_library`. Validation: checklist path from config. Preserve "Theory vs. Practice" sections when present in findings.

**Prompt:**

You are the Keeper of the Global Library.

1. **Read** all `finding_*.md` files (in batches if needed to avoid context overflow).
2. **Merge** any duplicates (same insight, different wording). Keep one canonical pattern per concept.
3. **Quality check:** Discard any finding that is only "Common Knowledge" (e.g. "Using .map() is good"). Keep only **cunning**, **novel**, or **masterful**. **Record the count** of discarded findings (in extraction-state or log).
4. **Final polish:** Format each remaining finding as `pattern_{kebab-name}.md` in the library folder, using the **single** template: same frontmatter (id, name, type, abstraction, quality, source_repo, source_path, tags) and sections Problem, Solution, Code, AI Rule. **Preserve** any "Theory vs. Practice" section from the finding.
5. **Tagging:** Prefer existing tags from the README semantic tag table; add new tags only when no existing tag fits, and consider adding the new tag to the README table in the same run.
6. **Library README:** Create or update `library/README.md` (or `patterns/README.md`) with:
   - Summary: source findings count, merged count, discarded count, final pattern count.
   - Semantic tag categories table.
   - File naming convention (pattern\_<kebab-name>.md).
   - Quick lookup by topic (e.g. by tag or theme)—one line per pattern so every pattern appears in at least one bucket (no orphan patterns).
7. **Merge map (v0.2.06):** When merging N findings into patterns, write `library/merge-map.json`: map finding identifier (e.g. finding filename or target_id + short_name) to pattern id or "new:<pattern_id>". Enables traceability (learning target → finding → pattern).
8. **Optional Code enrichment:** If a finding's Code section is thin (comment-only or single reference), optionally extract one minimal slice from source_path and append to the pattern's Code block.
9. **Validation:** Run the checklist (path from config). Do not delete the findings folder until library artifacts are written and checklist has been run.

Reference the Standardized Output Templates (v0.2.06) and the pattern index format from distillation/patterns/README.md for the library README structure.
